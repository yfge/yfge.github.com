---
layout: post
title: vibe‑coding 九阳神功之测：自动测试，交付有证据
permalink: /jy/ce/
tags: [AI, vibe-coding, 九阳神功]
---
# vibe‑coding 九阳神功之**测**：自动测试，交付有证据

> 系列顺序：[夯（Git）](/jy/git/) → [抄](/jy/chao/) → [学](/jy/xue/) → [喂](/jy/wei/) → [规](/jy/gui/) → [验](/jy/yan/) → **测** → [扩](/jy/kuo/) → [收](/jy/shou/)
> 一句话：**不是"我觉得能跑"，是用测试证明能跑。AI 写代码快，写测试也快——让它把证据一起交了。**

AI 编程现在最大的坑不是"不会写"，而是：

- 写得太快
- 改得太多
- 炸了没后路
- 炸完你甚至不知道它到底改了哪些文件

所以我准备写一套 **vibe‑coding 九阳神功**：核心不是提示词，而是把 AI 变成"可控交付的队友"。

这套"九阳神功"我先定了九个字诀：

- **夯**：Git 生存技能（刹车 / 保险 / 录像）
- **抄**：拆解 `yfge/orion`、`ai-shifu/ai-shifu` 的优秀骨架，搭底盘
- **学**：学习行业黑话，会组合技术栈、不懂原理也能不翻车
- **喂**：把 API 文档 / 资料结构化成可执行知识
- **规**：让 AI 出计划 + `tasks.md`，按清单推进
- **验**：多模型交叉验证，专治幻觉
- **测**：自动测试，交付有证据
- **扩**：扩展认知边界，但必须可验证
- **收**：上线交付、监控回滚、复盘闭环

这篇咱们讲第七式 **测**：**让 AI 不光写代码，还要写测试——交付的时候代码和证据一起交。**

---

## 为什么 vibe‑coding 更需要测试？

传统开发里，代码是你自己写的，你心里有数哪里可能出问题。

vibe‑coding 里，代码是 AI 写的。你对它的实现细节没有"直觉"。这意味着：

- 你不知道它有没有处理边界情况
- 你不知道它有没有偷偷引入副作用
- 你不知道它下次改的时候会不会把之前的功能搞坏

**测试就是你的"信任契约"**：AI 写的代码，必须附带测试；测试通过，才算交付。

---

## 让 AI 同时交付代码和测试

核心心法：**写代码的 prompt 里，永远带上"顺便写测试"。**

### 基础模板

```text
请实现【功能描述】。

要求：
1) 实现代码放到 src/xxx
2) 测试代码放到 tests/xxx
3) 测试覆盖：正常路径 + 至少 2 个异常路径
4) 给我运行测试的命令
```

### 实战：Orion 消息发送模块

```text
请实现 Orion 的消息发送模块 src/services/sender.py：
- 支持邮件、Slack、飞书三个渠道
- 发送失败自动重试（最多3次，指数退避）
- 所有渠道发送失败时记录到 failed_messages 表

同时写测试 tests/test_sender.py：
- 测试正常发送（mock 外部 API）
- 测试重试机制（模拟第1次失败、第2次成功）
- 测试全部失败时的降级处理
- 测试并发发送不会重复

运行命令：pytest tests/test_sender.py -v
```

**关键**：你不需要自己想测试用例，让 AI 想。但你要告诉它"至少覆盖哪些场景"。

---

## 三层测试策略：从快到慢

在 vibe‑coding 中，我把测试分三层，按成本从低到高：

### 第一层：冒烟测试（Smoke Test）——30 秒验证基本能跑

**目的**：服务能启动、核心接口能响应

```bash
# 最简单的冒烟测试
docker compose up -d
sleep 5
curl -f http://localhost:8080/api/healthz || echo "FAILED"
curl -f http://localhost:8080/api/version || echo "FAILED"
```

**让 AI 生成**：
```text
请给项目写一个冒烟测试脚本 scripts/smoke-test.sh：
1) 启动服务
2) 检查 health 接口
3) 检查核心 API 能不能返回 200
4) 任何一个失败就 exit 1
```

### 第二层：单元测试——验证每个函数/方法的行为

```text
请给 src/services/notification.py 写单元测试：
- mock 所有外部依赖（数据库、HTTP 请求）
- 覆盖正常路径和异常路径
- 每个测试方法名用中文注释说明测什么

运行：pytest tests/unit/ -v
```

**AI 写单元测试的产出示例**：

```python
import pytest
from unittest.mock import patch, MagicMock
from src.services.notification import send_notification

class TestSendNotification:
    def test_发送邮件_正常(self):
        """正常发送邮件应该返回 success"""
        with patch('src.services.notification.smtp_send') as mock_smtp:
            mock_smtp.return_value = True
            result = send_notification(channel='email', to='test@example.com', content='hello')
            assert result['status'] == 'success'

    def test_发送失败_自动重试(self):
        """第一次失败第二次成功应该返回 success"""
        with patch('src.services.notification.smtp_send') as mock_smtp:
            mock_smtp.side_effect = [Exception('timeout'), True]
            result = send_notification(channel='email', to='test@example.com', content='hello')
            assert result['status'] == 'success'
            assert mock_smtp.call_count == 2

    def test_全部失败_记录到数据库(self):
        """重试3次都失败应该记录到 failed_messages"""
        with patch('src.services.notification.smtp_send') as mock_smtp, \
             patch('src.services.notification.save_failed_message') as mock_save:
            mock_smtp.side_effect = Exception('timeout')
            result = send_notification(channel='email', to='test@example.com', content='hello')
            assert result['status'] == 'failed'
            mock_save.assert_called_once()
```

### 第三层：端到端测试——模拟用户操作验证完整流程

这一层最重，但对关键业务路径必须有。

```text
请写一个端到端测试 tests/e2e/test_user_flow.py：
- 注册新用户
- 登录拿到 token
- 创建一条通知规则
- 触发通知
- 验证通知记录存在

用 requests 库直接调 API，不要 mock。
前提：docker compose up -d 已经跑起来了。
```

---

## 让 AI 写验收脚本：交付物 = 代码 + 测试 + 验收脚本

我在做 fh_java 项目时养成了一个习惯：每次让 AI 交付一个功能，必须同时给三样东西：

1. **代码**：src/ 下的实现
2. **测试**：tests/ 下的自动化测试
3. **验收脚本**：scripts/verify-xxx.sh

验收脚本的模板：

```bash
#!/bin/bash
# scripts/verify-user-management.sh
# 验收：用户管理功能

set -e
echo "=== Phase 1: 冒烟测试 ==="
curl -sf http://localhost:8080/api/healthz > /dev/null
echo "✅ 服务正常"

echo "=== Phase 2: 注册 ==="
REGISTER=$(curl -s -X POST http://localhost:8080/api/register \
  -H 'Content-Type: application/json' \
  -d '{"email":"test@test.com","password":"Test1234!"}')
echo "$REGISTER" | jq -e '.id' > /dev/null
echo "✅ 注册成功"

echo "=== Phase 3: 登录 ==="
LOGIN=$(curl -s -X POST http://localhost:8080/api/login \
  -H 'Content-Type: application/json' \
  -d '{"email":"test@test.com","password":"Test1234!"}')
TOKEN=$(echo "$LOGIN" | jq -r '.token')
[ "$TOKEN" != "null" ] && echo "✅ 登录成功" || (echo "❌ 登录失败" && exit 1)

echo "=== Phase 4: 获取用户信息 ==="
ME=$(curl -s http://localhost:8080/api/me -H "Authorization: Bearer $TOKEN")
echo "$ME" | jq -e '.email' > /dev/null
echo "✅ 获取用户信息成功"

echo ""
echo "🎉 全部验收通过！"
```

**让 AI 生成这个脚本的提问**：
```text
请根据 tasks.md 的验收标准，生成 scripts/verify-user-management.sh：
- 用 curl + jq 验证每个接口
- 每步打印✅或❌
- 任何一步失败就 exit 1
- 最后输出总结
```

---

## 测试的 3 个实用技巧

### 技巧 1：让 AI 先写测试，再写代码（TDD 风格）

```text
我要实现消息重试机制。

请先写测试（tests/test_retry.py），定义好期望行为：
- 正常发送成功
- 失败后重试成功
- 超过重试次数后放弃
- 重试间隔符合指数退避

测试先写好，然后再写实现代码让测试通过。
```

这样做的好处：测试定义了"什么算对"，然后 AI 照着"对的标准"写代码，翻车率大降。

### 技巧 2：每个 commit 必须测试通过

在 tasks.md 的工作流里加一条规则：

```text
Phase 完成后，在 commit 之前：
1) 运行 pytest tests/ -v（或对应的测试命令）
2) 全部通过才 commit
3) 不通过就修，修到通过为止
```

### 技巧 3：用 CI 兜底

如果你有 GitHub Actions（或其他 CI），加个最简单的：

```yaml
# .github/workflows/test.yml
name: Test
on: [push, pull_request]
jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: '3.11'
      - run: pip install -r requirements.txt
      - run: pytest tests/ -v
```

这样即使你忘了跑测试，CI 也会帮你兜住。

---

## 常见问题

### "AI 写的测试质量靠谱吗？"

说实话，80 分。AI 写的测试覆盖了大部分正常路径，但经常遗漏：
- 并发场景
- 边界值（空字符串、超大数字、特殊字符）
- 状态依赖（测试执行顺序影响结果）

**对策**：让 AI 写完后，你补一句：
```text
请检查这些测试，补充以下场景：
1) 并发场景（两个请求同时到达）
2) 边界值（空输入、超长输入、特殊字符）
3) 确保测试之间没有状态依赖（每个测试独立运行）
```

### "所有代码都需要写测试吗？"

不需要。我的原则：
- **核心业务逻辑**：必须有测试（支付、认证、数据处理）
- **API 接口**：至少有冒烟测试
- **UI 页面**：靠 e2e 测试覆盖主路径就行
- **工具脚本**：跑一次看结果就行，不用专门写测试

---

## 和"规"的配合：tasks.md 里写测试标准

在 tasks.md 里，每个 Phase 的验收标准应该是**可执行的测试命令**：

```markdown
## Phase 2: API 层
- [ ] POST /api/register
- [ ] POST /api/login
- [ ] GET /api/me
- [ ] 测试：pytest tests/test_auth.py -v 全部通过
- [ ] 验收脚本：bash scripts/verify-auth.sh 输出 🎉
```

这样"做完了"就有了清晰的定义：不是"AI 说做完了"，而是"测试全绿了"。

---

## 总结

"测"的核心就一句话：**不是"我觉得能跑"，而是用测试证明能跑。**

- AI 写代码快，写测试也快——让它把证据一起交
- 三层策略：冒烟测试（快）→ 单元测试（准）→ 端到端测试（全）
- 每个 commit 前跑测试，CI 兜底
- 交付物 = 代码 + 测试 + 验收脚本

下一式我们讲 **扩**：AI 能帮你学新技术，但学到的东西必须可验证。
