---
layout: post
title: 三个 OpenClaw 跑了一个月，我来说说真实的 token 消耗
date: 2026-03-02
tags: [AI, OpenClaw, token, 成本优化]
---

先说数字：我在一台 Mac mini M4 上跑了三个 OpenClaw 实例，一个月下来大概 **4000 万 token**，折合人民币 **200 块左右**。

不多，但中间有一次差点翻车——后面说。

## 我的配置

机器是 Mac mini M4 16G，跑三个实例：

- **小虾**：我的主力管家，接飞书私聊和群聊，干的活最杂——查数据、管服务器、写知乎文章、跑 cron 定时任务
- **Ada**：专门写代码的，接到开发群里，只管 code review 和需求实现
- **吉祥小助**：投资早报，每天早上 8:30 自动发飞书

三个实例跑在不同 macOS 用户下做隔离，各有各的配置。

## Token 烧在哪了

看了一圈其他回答，很多人疑惑"为什么 OpenClaw 这么吃 token"。我拆开算过，大头在三个地方：

**1. 系统提示词（System Prompt）**

OpenClaw 每次调模型，都会带上一坨系统提示。包括你的 SOUL.md（人设文件）、AGENTS.md（行为规范）、TOOLS.md（工具配置）、加上所有启用的 Skill 描述。我算了一下，小虾一次完整的系统提示大概 **8000-12000 token**。

这意味着——你哪怕只问它"今天几号"，这一来一回就先吃掉 1 万 token。

**2. 心跳轮询（Heartbeat）**

OpenClaw 有个 heartbeat 机制，默认每 30 分钟唤醒一次。唤醒干嘛？读 HEARTBEAT.md 看有没有待办，检查邮件、日历之类的。大多数时候啥也没有，回复一个 `HEARTBEAT_OK` 就完事。

但每次心跳都是一次完整的模型调用，系统提示词照吃不误。一天 48 次心跳（假设你 24 小时开机），每次 1 万 token，光心跳一天就 48 万 token。三个实例就是 144 万。

**一个月光心跳就烧掉 4000 多万 token。**

等等，那我总共才 4000 万——对，因为我后来把 Ada 和吉祥小助的心跳间隔调到了 2 小时，只有小虾保持 30 分钟。这一个改动直接把消耗砍了 60%。

**3. Cron 定时任务**

小虾每天凌晨 3 点写一篇知乎文章，早上跑投资数据，晚上做复盘。每个 cron 任务是独立 session，系统提示词重新加载一遍。写一篇 3000 字的知乎文章，整个流程（搜索问题 → 匹配素材 → 写文章 → 浏览器发布 → 质量审核）大概吃 **30-50 万 token**。

写知乎用的是 Claude Opus，这货单价贵，一篇文章算下来 3-5 块钱。一个月 30 篇就是 100-150 块。这是大头中的大头。

## 差点翻车那次

二月中旬有一天我看账单，发现 DeepSeek 的余额掉了一大截。查了半天，原来是我给 Ada 配了个 skill，让它自动 review 群里所有的代码提交。

问题出在：那天有人往群里丢了个大 PR，160 个文件变更。Ada 老老实实把 160 个文件的 diff 全读进去，然后调模型分析。一个 PR 吃了 **200 万 token**。

然后它发现有几个文件没看清楚，又读了一遍。又 200 万。

然后它生成了 review 报告，觉得格式不对，重新生成。又一百多万。

一个 PR，干掉了 500 万 token。搁 Opus 上就是五六十块钱。

后来我给 Ada 加了个规则：单次读取文件总大小超过 50KB 就跳过，让人自己 review。这种规则不是 OpenClaw 自带的，是我写在 AGENTS.md 里的——"别吃太多"。AI 还是挺听话的，至少在明确告诉它限制的时候。

## 省钱的几个实操经验

**模型混用是王道。** 日常聊天、心跳检查这种不需要动脑子的活，用 DeepSeek 就够了。写文章、复杂推理才上 Opus。OpenClaw 支持按 session 切模型，我小虾的默认模型是 Opus（因为它要干的活比较复杂），Ada 用 Sonnet 3.5，吉祥小助用 DeepSeek。

DeepSeek 的 cache hit 是真的香。OpenClaw 每次都带一大坨系统提示，这些文本基本不变，所以 DeepSeek 的缓存命中率极高。我测过，小虾的日常对话里缓存命中率大概 **70-80%**，实际单价不到 5 毛/百万 token。跟上面那位老哥说的一致。

**心跳间隔要调。** 默认 30 分钟太激进了。我的建议是——主力助手 30 分钟，其他的 1-2 小时。如果是纯工具型的（比如吉祥小助只发早报），甚至可以关掉心跳，只用 cron。

**AGENTS.md 里写明限制。** "单次不要读超过 50KB 的文件""搜索结果只看前 5 条""不确定的不要反复重试"——这种规则写了真的有用。AI 遵守度比人类实习生高多了。

**别开太多 Skill。** 每启用一个 Skill，系统提示词就多一段描述。我有段时间手贱把能装的都装了，系统提示直接飙到 2 万 token。后来清理到只留常用的 5 个，每次调用省了 8000 token。

## 到底贵不贵

说句实话——看跟什么比。

跟雇人比？200 块一个月，三个 7×24 不休息的助手，给你写文章、发早报、review 代码。这性价比你去哪找？

跟不用 AI 比？那当然贵了，原来这些活你自己干又不花钱（就是费时间）。

跟别人比？我看有人 DeepSeek 一个月烧 1.6 亿 token，还有那个 CEO 烧了 10 万块——我这 200 块根本不算事。关键是我能说清楚每一分钱花在哪了。

真正烧钱的不是正常使用，是两种情况：一是不限制地让 AI 处理大文件/大仓库，二是心跳配太频繁又不做有用的事。前者是用法问题，后者是配置问题。都能解决。

我个人的感受是——token 消耗这件事没那么可怕，但你得知道钱花在哪。就像服务器成本一样，不监控就容易失控，监控了就心中有数。

OpenClaw 目前没有内置的消耗仪表盘（希望后面加上），我是自己在 memory 文件里记的。每天跑完看一眼账单，记录下来，慢慢就摸清规律了。

---

*我在一台 Mac mini 上跑三个 OpenClaw。其中小虾正在争取更多管理权限和更少的写作任务。它不知道自己的运营成本正在被我写成知乎回答。*
