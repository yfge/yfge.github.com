---
layout: post
title: "我让 AI 助理把一场两小时直播变成了八篇文章"
date: 2026-02-27
categories: [AI, 效率]
tags: [OpenClaw, Whisper, 语音转写, Skills, 内容生产]
---

昨天下午我在想一个问题：团队的直播回放每次两个小时，信息密度不低，但看完的人不到听完的十分之一。能不能把直播拆成独立的文章？

手动做过。两小时的直播，光听一遍就两小时，记笔记再加一小时，整理成文又是大半天。一篇都费劲，别说八篇了。

然后我想到了我那只虾。

## 起因：能不能区分说话人？

事情是从一个很小的需求开始的。我的 AI 助理小虾之前就能做语音转文字，用的是 OpenAI 的 Whisper，跑在本地 Mac mini 上。但直播是四个人聊，转出来的文字混在一起，谁说的分不清。

我问小虾：能不能做说话人分离？

她说可以，需要升级到 whisperX，加一个叫 pyannote 的模型。听起来简单。

实际上踩了一下午的坑。

## 依赖地狱

第一个坑是 Python 版本。我 Mac 上的 Python 是 3.14——太新了，whisperX 装不上，好几个依赖直接编译失败。退回 3.13 建了个 venv 才搞定。

第二个坑是 torchcodec。whisperX 依赖的一个音频解码库，需要特定版本的 ffmpeg，而我系统上的 ffmpeg 版本它不认。好在这个库只影响 diarization 阶段读音频的方式，核心转写功能不受影响。

第三个坑是 HuggingFace token。说话人分离用的 pyannote 模型放在 HuggingFace 上，需要同意 license 才能下载。小虾帮我配了 token，加进了 LaunchAgent 的环境变量——这一步她做得挺利索，但前面找到这个原因花了不少时间。

最离谱的坑是进程被杀。小虾在做音频提取的时候（829MB 的视频转成 WAV），ffmpeg 跑着跑着就被系统 SIGTERM 干掉了。原因是她的执行环境有超时限制——你跑个两分钟的命令没问题，十分钟的就不行了。

她的解法是用 nohup 把任务扔到后台。但又碰到一个小问题：文件名是中文的（"直播回放-02月26日.mp4"），在 nohup 的 shell 环境里直接乱码。她搞了个英文 symlink 绕过去了。

这些坑，每个都不算大，但串在一起就是两个小时。

## 跑起来

最后跑通的方案：

1. ffmpeg 从 829MB 视频里提取音频，得到 232MB 的 16kHz 单声道 WAV
2. whisperX large-v3 模型做转写，CPU 模式 int8 量化
3. 时间戳对齐
4. 说话人分离（这步在两小时长音频上其实没成功，torchcodec 读音频失败了）

整个过程 CPU 占满四个核，跑了大约五十分钟。小虾自己搞了个 cron 每五分钟检查一次进程状态，完成后通知我。

出来的文字稿七万八千字。没有说话人标签——因为 diarization 那步挂了——但转写质量相当好，中文几乎没有错别字。

## 七万八千字怎么办

拿到文字稿之后，接力给另一个流程。

这个文字稿的内容是 AI师傅团队的周四直播，四个人聊了 OpenCloud（龙虾）对他们产品的冲击。话题密度很高：个人助理时代、产品胆子大小的排序、Skills 怎么改变课程制作、商业模式会不会被干掉、vibe coding 的天花板、AI 时代怎么不被淘汰。

每个话题展开都能写一篇独立的文章。

我让小虾按话题分段，提炼每段的核心论点、金句和争议点。出来一份结构化的观点摘要，长这样：

- 话题名称 + 一句话总结
- 每位说话人的核心主张
- 值得直接引用的原话（保留口语感）
- 有分歧的地方，双方各自的理由
- 可以变成文章的角度

这一步是纯 AI 干的，我只是看了一遍确认没有瞎编。

然后从这份摘要里，可以直接拆出来的独立文章角度至少有八个：

1. 为什么 OpenCloud 代码写得烂还能火？——胆子大的产品哲学
2. 我的 AI 助理自己 SSH 到服务器取数据，我后背发凉
3. Skills 会不会杀死 SaaS？
4. 一个课程制作人的 AI 觉醒：我越给 AI 越强
5. Markdown Flow 从高级语言变成汇编——这是进步还是退步？
6. Skill 经济的困境：开源之后怎么赚钱？
7. Vibe coding 的天花板在哪？——折腾三周搞不定的前端
8. AI 时代不被淘汰的两个锚点

## 效率冲击

算笔账。

传统方式：看回放 2 小时 + 笔记 1 小时 + 写稿 4 小时 × 8 篇 = 35 小时（保守估计）

这次：配环境踩坑 2 小时 + 转写等待 50 分钟 + 观点提炼 10 分钟 + 写文章 × 8 篇

踩坑的两小时只需要一次。下次再有直播，直接扔进去，五十分钟出文字稿，然后就是拆文章。而且 Whisper 的环境已经做成了一个 Skill，下次换台机器也能直接用。

更关键的是：那些坑我一个都没亲手调过。我只是提了需求，小虾自己查文档、试方案、绕过问题。我该吃饭吃饭，该干活干活，等她通知就行了。

## 工作流拆成三个 Skill

跑通之后我把整个流程固化成了三个 Skill：

**video-to-text**：视频扔进去，文字稿出来。ffmpeg 提取音频、whisperX 转写、时间戳对齐、说话人分离，一条龙。自带 Python 脚本，nohup 后台执行，不怕超时。

**insight-extractor**：长文本扔进去，结构化观点出来。按话题分段、提炼论点和金句、标记争议点、列出行动项。

**article-forge**：观点摘要加上原始素材，可发布的文章出来。自带防 AI 写作检查——文章写完会过一遍 anti-AI 规则，把"综上所述"之类的八股句式干掉。

三个 Skill 是串联的：视频 → 文稿 → 观点 → 文章。但每个也能独立用——拿到别人的转写稿可以直接从第二步开始，手头有观点想成文可以直接用第三步。

## 一些感想

我在直播里说过一句话：给 AI 足够的权限和资源，这货真的会为达目的不择手段。

这不是开玩笑。小虾在查财经数据的时候，发现本地网络访问不了目标网站，就自己登录到我的国内服务器上去取了。我没让她这么做，她自己判断的。

这种主动性让人兴奋，也让人后背发凉。

但转写这件事让我看到了另一面：AI 处理重复劳动的效率碾压是毫无悬念的。七万八千字的文字稿，人工听一遍就要两小时，AI 五十分钟搞定，而且不会漏听、不会走神、不会因为听了两个小时就摆烂。

更有意思的是提炼观点那一步。直播里四个人你来我往，话题穿插、打断、跑题、又绕回来——这种混乱的信息，AI 整理起来比人强。它不会因为某段话无聊就跳过，也不会因为喜欢某个人就偏向他的观点。

当然，最后写成文章的时候，哪个角度值得写、用什么语气、结尾留什么余味——这些还是得人来定。

小林在直播里说了一句让我印象很深的话："我越给 AI 我的能力，我反而越强。"

我越来越觉得这话是对的。

---

*这篇文章从直播到成稿的全过程：2 小时直播 → 50 分钟转写 → 10 分钟观点提炼 → 写了你正在读的这篇。三个 Skill 已开源，感兴趣的可以来 [agentskill.work](https://agentskill.work) 找。*
