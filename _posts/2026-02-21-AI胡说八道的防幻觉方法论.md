---
layout: post
title: AI 胡说八道不是 bug，是工作原理：一个老程序员被坑三次后的防幻觉方法论
tags: [AI, vibe-coding, 幻觉, 九阳神功]
---

# 因为 AI 的"幻觉"是它最底层的工作原理，不是 bug

先说结论：**AI 胡说八道不是偶尔犯错，而是它的工作原理决定了它必然会犯错。理解这一点，是你能正确使用 AI 的前提。**

我是一个写了十几年代码的老程序员，过去一年天天跟 AI 打交道（Claude Code、GPT、Codex）。被 AI 坑的次数多了，我总结出了一套防幻觉的方法论。今天不讲理论，讲真实案例和实战解法。

---

## 一、先讲三个我被 AI 坑的真实案例

### 案例 1：不存在的 API

在做一个飞书登录集成时，GPT-4 告诉我飞书有个 `/open-apis/auth/v3/user_access_token` 接口。我照着写完了，一跑——404。

查了官方文档，v3 根本不存在，实际是 v1。

**AI 编了一个看起来很合理的 URL。** 它不是故意骗你，它只是在"合理推测"下一个 token 应该是什么。

### 案例 2：离谱的配置参数

让 Claude 帮我写 Nginx 配置，它给了一个 `proxy_buffer_size 128m` 的建议。看起来很专业，对吧？

实际上这个值大得离谱，生产环境会直接吃光内存。AI 只是在"合理范围内"选了一个数字，但它不知道你的服务器只有 2G 内存。

### 案例 3：过时的写法

让 AI 写 Docker Compose 配置，它用了 `version: '3.8'`。这个写法在新版 Docker Compose 里已经废弃了，虽然能跑，但会一直报 warning。

AI 的训练数据有截止日期，它不知道"现在"是什么时候。

---

## 二、为什么 AI 必然会胡说八道

技术上说，大语言模型的工作原理是：**根据上文，预测下一个最可能的 token（词/字）。**

它不是在"思考"，不是在"查资料"，更不是在"理解"。它只是在做一件事：

> 给定前面的文字，下一个字大概率是什么？

所以当你问它一个它训练数据里没见过的东西，它不会说"我不知道"——它会**继续预测下一个看起来合理的 token**。

这就是幻觉的本质：**AI 宁可编一个看起来对的答案，也不会说"我不确定"。**

因为"说不确定"在它的训练目标里，不是一个高概率的输出。

---

## 三、怎么防？我的实战方法论

### 方法 1：多模型交叉验证（最有效）

**用 Claude 验 GPT，用 GPT 验 Claude，让它们互相打脸。**

具体操作：
1. 用 Claude 生成代码/方案
2. 把同样的需求丢给 GPT（或 Qwen、Gemini）
3. 如果两个模型的回答一致 → 可信度高
4. 如果不一致 → 红旗！仔细查

我做 ZhiForge（一个自动化知乎写作工具）时，每篇文章写完都会用另一个模型做质量评审，7 个维度打分。不是信不过写文章的模型，而是**一个模型的盲区往往是另一个模型的强项**。

### 方法 2：让 AI 给出验证命令

别只要代码，要求 AI 同时给出"怎么验证这段代码是对的"。

```
你：帮我写一个 Redis 缓存的配置
AI：好的，这是配置...

你：给我一条命令，验证这个配置是否生效
AI：redis-cli CONFIG GET maxmemory
```

**能给出验证命令的回答，可信度翻倍。** 因为验证命令本身也可以被执行和检验。

### 方法 3：怀疑一切 URL 和版本号

这是我被坑最多的地方。AI 给的 URL、API 路径、版本号，**至少有 30% 是编的**。

铁律：
- API 路径 → 去官方文档核实
- 库版本号 → `npm info xxx version` 或 `pip show xxx`
- 配置参数 → 查官方文档的默认值和推荐值

### 方法 4：小步执行，不要一次给它 20 个文件

AI 一次改太多文件，出了幻觉你根本不知道哪里出了问题。

我的铁律：
- 每次只让 AI 改一个功能点
- 改完立刻 `git diff` 看改了什么
- 跑一下确认没问题再继续
- 打一个 git commit 存档

这样即使 AI 胡说八道了，你也能快速定位和回退。

---

## 四、一个反直觉的结论

很多人觉得 AI 越来越强了，幻觉问题迟早会解决。

**我的判断恰恰相反：幻觉不会消失，只会变得更隐蔽。**

因为模型越强，它编的东西越像真的。GPT-3 编的 API 一眼就能看出来假的；GPT-5 编的 API 可能跟真的只差一个参数名。

这意味着：**你不能靠"感觉"判断 AI 的输出对不对，必须靠流程和工具。**

交叉验证、验证命令、小步执行——这些不是可选项，是必选项。

---

## 总结

| 问题 | 解法 |
|------|------|
| AI 编 API/URL | 查官方文档核实 |
| AI 给错配置参数 | 查文档默认值 + 交叉验证 |
| AI 用过时写法 | 问它"这个写法在 2026 年还适用吗？" |
| 不确定 AI 说的对不对 | 多模型交叉验证 |
| AI 改了太多文件 | 小步执行 + git diff |

**AI 胡说八道不是 bug，是 feature。** 理解这一点，你就不会被坑；不理解这一点，你迟早被坑。

和 AI 打交道的正确态度：**信任但验证（Trust but verify）。**
